{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "HeN9bkZR2-Pj",
        "aAjLRuoGA2_y",
        "DwZCqUDQzE56"
      ],
      "toc_visible": true,
      "name": "OpenLLM Workshop",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bentoml/workshops/blob/main/openllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <h1>OpenLLM</h1>\n",
        "        <img alt=\"BentoML logo\" src=\"https://raw.githubusercontent.com/bentoml/BentoML/main/docs/source/_static/img/bentoml-logo-black.png\" width=\"200\"/>\n",
        "        </br>\n",
        "        <a href=\"https://github.com/bentoml/OpenLLM\">GitHub</a>\n",
        "        |\n",
        "        <a href=\"https://l.bentoml.com/join-openllm-discord\">Community</a>\n",
        "    </p>\n",
        "</center>\n",
        "<h1 align=\"center\">Serving an open-source LLM with OpenLLM</h1>\n",
        "\n",
        "Open-source LLMs can be very powerful and flexible, but setting them up for serving can be quite difficult. Indeed, the vast landscape of LLMs means that you may have to do the same process for several models in order to evaluate their performance. With OpenLLM, you can adapt and serve many models (including Llama 2 and Falcon) with ease.\n",
        "\n",
        "In this tutorial, you will learn the following:\n",
        "\n",
        "- Set up your environment to work with OpenLLM.\n",
        "- Serve LLMs like Llama 2 with just a single command.\n",
        "- Explore different ways to interact with the OpenLLM server.\n",
        "- Advanced features of OpenLLM like quantization.\n",
        "- Integrate OpenLLM with LangChain.\n",
        "- Port existing OpenAI applications to use OpenLLM with minimal code changes.\n",
        "- Create a multi-modal application with OpenLLM and BentoML."
      ],
      "metadata": {
        "id": "Ae3XDcmOCraD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "LVSRXlTJteuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into OpenLLM, let's ensure our environment has everything in place.\n",
        "\n",
        "For the workshop, we will be using a remote OpenLLM server hosted on BentoCloud, as the resource limits of free Colab are quite small for LLMs.\n",
        "\n",
        "If you are running this after the workshop, you can provide the URL of your own server, or alternatively simply tick `RUN_IN_COLAB` to attempt to run a server in the Colab runtime."
      ],
      "metadata": {
        "id": "AKL2j904kqGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RUN_IN_COLAB = False #@param {type: 'boolean'}\n",
        "SERVER_URL = \"https://llm.b2.vc\" # @param {type: 'string'}\n",
        "\n",
        "print(\"Installing OpenLLM...\")\n",
        "!pip install --upgrade -q --progress-bar off openllm bentoml langchain openai\n",
        "print(\"Done!\")\n",
        "\n",
        "if RUN_IN_COLAB:\n",
        "  SERVER_URL = \"http://localhost:8001\"\n",
        "  print(\"Installing serving dependencies...\")\n",
        "  !pip install --upgrade -q --progress-bar off openllm[llama,vllm,gptq] tensorrt accelerate bitsandbytes\n",
        "  !apt install tensorrt\n",
        "  print(\"Done!\")\n",
        "\n",
        "  print(\"Downloading Llama and starting OpenLLM server...\")\n",
        "  !nohup openllm start llama --model-id \"NousResearch/llama-2-7b-chat-hf\" --backend vllm --port 8001 > openllm.log 2> openllm.err &\n",
        "\n",
        "import os\n",
        "os.environ[\"SERVER_URL\"] = SERVER_URL\n"
      ],
      "metadata": {
        "id": "vfbUDJzFtke-",
        "outputId": "2d46de64-c853-4cf1-eb11-4deaab3cc8fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing OpenLLM...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serve Llama 2!\n",
        "\n",
        "While we won't be serving Llama 2 from inside the Colab environment, serving it is straightforward with OpenLLM. With just a single command, you're good to go:\n",
        "\n",
        "```bash\n",
        "openllm start llama --model-id \"NousResearch/llama-2-7b-chat-hf\" --backend vllm --port 8001\n",
        "```\n",
        "\n",
        "If you have access to a GPU environment, try it out!"
      ],
      "metadata": {
        "id": "UOrB4TU20Zl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we're serving the smallest Llama 2 model, the 7 billion parameter version. You can simply change `7b` in the string with `13b` or `70b` for the larger parameter sizes."
      ],
      "metadata": {
        "id": "bimBS4uv2UBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View OpenLLM model options"
      ],
      "metadata": {
        "id": "HeN9bkZR2-Pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenLLM offers a wide range of models and backends. To see the available options, simply run `openllm models`:"
      ],
      "metadata": {
        "id": "HlsRN4TWmUpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openllm models"
      ],
      "metadata": {
        "id": "kvIBUYE-2YoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bbbab46-f7b2-4b76-d0eb-ee2bdaebbebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[37m╒═══════════╤══════════════════════╤════════════════════════════════╤══════════════════════╤══════════════════════════════╕\n",
            "│ LLM       │ Architecture         │ Models Id                      │ Installation         │ Runtime                      │\n",
            "╞═══════════╪══════════════════════╪════════════════════════════════╪══════════════════════╪══════════════════════════════╡\n",
            "│ chatglm   │ ChatGLMForConditiona │ ['thudm/chatglm-6b',           │ \"openllm[chatglm]\"   │ ('pt',)                      │\n",
            "│           │ lGeneration          │ 'thudm/chatglm-6b-int8',       │                      │                              │\n",
            "│           │                      │ 'thudm/chatglm-6b-int4',       │                      │                              │\n",
            "│           │                      │ 'thudm/chatglm2-6b',           │                      │                              │\n",
            "│           │                      │ 'thudm/chatglm2-6b-int4']      │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ dolly-v2  │ GPTNeoXForCausalLM   │ ['databricks/dolly-v2-3b',     │ openllm              │ ('pt', 'vllm')               │\n",
            "│           │                      │ 'databricks/dolly-v2-7b',      │                      │                              │\n",
            "│           │                      │ 'databricks/dolly-v2-12b']     │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ falcon    │ FalconForCausalLM    │ ['tiiuae/falcon-7b',           │ \"openllm[falcon]\"    │ ('pt', 'vllm')               │\n",
            "│           │                      │ 'tiiuae/falcon-40b',           │                      │                              │\n",
            "│           │                      │ 'tiiuae/falcon-7b-instruct',   │                      │                              │\n",
            "│           │                      │ 'tiiuae/falcon-40b-instruct']  │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ flan-t5   │ T5ForConditionalGene │ ['google/flan-t5-small',       │ \"openllm[flan-t5]\"   │ ('pt', 'flax', 'tf')         │\n",
            "│           │ ration               │ 'google/flan-t5-base',         │                      │                              │\n",
            "│           │                      │ 'google/flan-t5-large',        │                      │                              │\n",
            "│           │                      │ 'google/flan-t5-xl',           │                      │                              │\n",
            "│           │                      │ 'google/flan-t5-xxl']          │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ gpt-neox  │ GPTNeoXForCausalLM   │ ['eleutherai/gpt-neox-20b']    │ openllm              │ ('pt', 'vllm')               │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ llama     │ LlamaForCausalLM     │ ['meta-llama/Llama-2-70b-chat- │ \"openllm[llama]\"     │ ('pt', 'vllm')               │\n",
            "│           │                      │ hf', 'meta-                    │                      │                              │\n",
            "│           │                      │ llama/Llama-2-13b-chat-hf',    │                      │                              │\n",
            "│           │                      │ 'meta-llama/Llama-2-7b-chat-   │                      │                              │\n",
            "│           │                      │ hf', 'meta-                    │                      │                              │\n",
            "│           │                      │ llama/Llama-2-70b-hf', 'meta-  │                      │                              │\n",
            "│           │                      │ llama/Llama-2-13b-hf', 'meta-  │                      │                              │\n",
            "│           │                      │ llama/Llama-2-7b-hf', 'NousRes │                      │                              │\n",
            "│           │                      │ earch/llama-2-70b-chat-hf', 'N │                      │                              │\n",
            "│           │                      │ ousResearch/llama-2-13b-chat-  │                      │                              │\n",
            "│           │                      │ hf',                           │                      │                              │\n",
            "│           │                      │ 'NousResearch/llama-2-7b-chat- │                      │                              │\n",
            "│           │                      │ hf',                           │                      │                              │\n",
            "│           │                      │ 'NousResearch/llama-2-70b-hf', │                      │                              │\n",
            "│           │                      │ 'NousResearch/llama-2-13b-hf', │                      │                              │\n",
            "│           │                      │ 'NousResearch/llama-2-7b-hf']  │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ mpt       │ MPTForCausalLM       │ ['mosaicml/mpt-7b',            │ \"openllm[mpt]\"       │ ('pt', 'vllm')               │\n",
            "│           │                      │ 'mosaicml/mpt-7b-instruct',    │                      │                              │\n",
            "│           │                      │ 'mosaicml/mpt-7b-chat',        │                      │                              │\n",
            "│           │                      │ 'mosaicml/mpt-7b-storywriter', │                      │                              │\n",
            "│           │                      │ 'mosaicml/mpt-30b',            │                      │                              │\n",
            "│           │                      │ 'mosaicml/mpt-30b-instruct',   │                      │                              │\n",
            "│           │                      │ 'mosaicml/mpt-30b-chat']       │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ opt       │ OPTForCausalLM       │ ['facebook/opt-125m',          │ \"openllm[opt]\"       │ ('pt', 'flax', 'tf', 'vllm') │\n",
            "│           │                      │ 'facebook/opt-350m',           │                      │                              │\n",
            "│           │                      │ 'facebook/opt-1.3b',           │                      │                              │\n",
            "│           │                      │ 'facebook/opt-2.7b',           │                      │                              │\n",
            "│           │                      │ 'facebook/opt-6.7b',           │                      │                              │\n",
            "│           │                      │ 'facebook/opt-66b']            │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ stablelm  │ GPTNeoXForCausalLM   │ ['stabilityai/stablelm-tuned-  │ openllm              │ ('pt', 'vllm')               │\n",
            "│           │                      │ alpha-3b',                     │                      │                              │\n",
            "│           │                      │ 'stabilityai/stablelm-tuned-   │                      │                              │\n",
            "│           │                      │ alpha-7b',                     │                      │                              │\n",
            "│           │                      │ 'stabilityai/stablelm-base-    │                      │                              │\n",
            "│           │                      │ alpha-3b',                     │                      │                              │\n",
            "│           │                      │ 'stabilityai/stablelm-base-    │                      │                              │\n",
            "│           │                      │ alpha-7b']                     │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ starcoder │ GPTBigCodeForCausalL │ ['bigcode/starcoder',          │ \"openllm[starcoder]\" │ ('pt', 'vllm')               │\n",
            "│           │ M                    │ 'bigcode/starcoderbase']       │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ baichuan  │ BaiChuanForCausalLM  │ ['baichuan-inc/baichuan-7b',   │ \"openllm[baichuan]\"  │ ('pt', 'vllm')               │\n",
            "│           │                      │ 'baichuan-                     │                      │                              │\n",
            "│           │                      │ inc/baichuan-13b-base',        │                      │                              │\n",
            "│           │                      │ 'baichuan-                     │                      │                              │\n",
            "│           │                      │ inc/baichuan-13b-chat',        │                      │                              │\n",
            "│           │                      │ 'fireballoon/baichuan-vicuna-  │                      │                              │\n",
            "│           │                      │ chinese-7b',                   │                      │                              │\n",
            "│           │                      │ 'fireballoon/baichuan-         │                      │                              │\n",
            "│           │                      │ vicuna-7b',                    │                      │                              │\n",
            "│           │                      │ 'hiyouga/baichuan-7b-sft']     │                      │                              │\n",
            "╘═══════════╧══════════════════════╧════════════════════════════════╧══════════════════════╧══════════════════════════════╛\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serving with quantization\n",
        "\n",
        "Unless you have a compute GPU with significants amounts of VRAM, you can't serve larger models, as the VRAM requirement is very large."
      ],
      "metadata": {
        "id": "aAjLRuoGA2_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [Optional] Try to serve Llama 13b\n",
        "!openllm start llama --model-id \"NousResearch/llama-2-13b-chat-hf\" --port 8001"
      ],
      "metadata": {
        "id": "28Ej1fiID-_i",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to serve 13b on a GPU with limited VRAM, we can take advantage of [quantization](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c). In essence, quantization rounds weight values, significantly lowering the VRAM required to hold the model in memory. You can start using quantization super easily with OpenLLM, by simply passing the `--quantize` argument. There are several options, including `int4` and `int8`, but for this workshop, we will be using the state-of-the-art [`gptq` quantization](https://arxiv.org/abs/2210.17323)."
      ],
      "metadata": {
        "id": "t8SuuAhTExkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openllm start llama --model-id \"TheBloke/Llama-2-13b-Chat-GPTQ\" --quantize gptq --serialization legacy"
      ],
      "metadata": {
        "id": "ZUGjrebmFUjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the OpenLLM server"
      ],
      "metadata": {
        "id": "tvWmNvppjU6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check server status"
      ],
      "metadata": {
        "id": "DwZCqUDQzE56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you interact with the OpenLLM server, it's crucial to ensure that it is up and running. The output of the `curl` command should start with `HTTP/1.1 200 OK`, meaning everything is in order.\n",
        "\n",
        "If it says `curl: (6) Could not resolve host: SERVER_URL`, ensure you have run the setup step.\n",
        "\n",
        "If it says `curl: (7) Failed to connect to localhost...`, then check `./openllm.log` and `./openllm.err`; likely the server has failed to start or is still in the process of starting.\n",
        "\n",
        "If it says `HTTP/1.1 503 Service Unavailable`, the server is still starting and you should wait a bit and retry."
      ],
      "metadata": {
        "id": "YmAl7HYgnDBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -i {SERVER_URL}/readyz"
      ],
      "metadata": {
        "id": "SPVPsGp2nFS5",
        "outputId": "c723a033-d0b2-45c6-8f39-2cc3ca68f614",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTTP/2 200 \r\n",
            "\u001b[1mdate\u001b[0m: Thu, 12 Oct 2023 23:54:59 GMT\r\n",
            "\u001b[1mcontent-type\u001b[0m: text/plain; charset=utf-8\r\n",
            "\u001b[1mcontent-length\u001b[0m: 1\r\n",
            "\u001b[1mstrict-transport-security\u001b[0m: max-age=15724800; includeSubDomains\r\n",
            "\u001b[1mx-powered-by\u001b[0m: Yatai\r\n",
            "\u001b[1mx-yatai-org-name\u001b[0m: unknown\r\n",
            "\u001b[1mx-yatai-bento\u001b[0m: meta-llama--llama-2-13b-chat-hf-service:v4-new-prompt\r\n",
            "\u001b[1mcf-cache-status\u001b[0m: DYNAMIC\r\n",
            "\u001b[1mreport-to\u001b[0m: {\"endpoints\":[{\"url\":\"https:\\/\\/a.nel.cloudflare.com\\/report\\/v3?s=br84OpSy120f0MHCC6lUYKJTTf%2Ffl3bp5tiycOXgFL2b4NijUersHA7jkkdYq48rTsnYgRNPYdkmzycgWKUkOUSJfYk4CGWm0%2FeQ9aPSqHb3b7j0ceDsvFaLWn8%3D\"}],\"group\":\"cf-nel\",\"max_age\":604800}\r\n",
            "\u001b[1mnel\u001b[0m: {\"success_fraction\":0,\"report_to\":\"cf-nel\",\"max_age\":604800}\r\n",
            "\u001b[1mserver\u001b[0m: cloudflare\r\n",
            "\u001b[1mcf-ray\u001b[0m: 81534564bf6eb0e5-ATL\r\n",
            "\u001b[1malt-svc\u001b[0m: h3=\":443\"; ma=86400\r\n",
            "\r\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Raw HTTP"
      ],
      "metadata": {
        "id": "o2J9PlcezIzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several ways you can interact with an OpenLLM server. Since it is a standard HTTP server that accepts JSON as input, you can simply use `cURL` (or any HTTP client of your choice):"
      ],
      "metadata": {
        "id": "Azr44vD_jc48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -k -X 'POST' \\\n",
        "  \"$SERVER_URL/v1/generate_stream\" \\\n",
        "  -H 'accept: text/event-stream' \\\n",
        "  -H 'Content-Type: application/json' \\\n",
        "  -d '{\"prompt\":\"write a tagline for an ice cream shop\\n\", \"llm_config\": {\"max_new_tokens\": 8192}}'"
      ],
      "metadata": {
        "id": "3rwArmgCm_j9",
        "outputId": "a584aca5-fe7e-4aa2-efd0-04e87063bf66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure! Here are some tagline options for an ice cream shop:\n",
            "\n",
            "1. \"Savor the Sweet Life\"\n",
            "2. \"Cool Treats for a Hot World\"\n",
            "3. \"Where Every Day is a Sundae\"\n",
            "4. \"Chill Out with Us\"\n",
            "5. \"The Creamery of Your Dreams\"\n",
            "6. \"A Scoop Above the Rest\"\n",
            "7. \"Taste the Joy in Every Bite\"\n",
            "8. \"Satisfy Your Cravings, Indulge in Our Delights\"\n",
            "9. \"The Perfect Treat for Any Mood\"\n",
            "10. \"Experience the Magic of Ice Cream\"\n",
            "\n",
            "I hope these tagline options inspire you and help you come up with the perfect one for your ice cream shop! "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenLLM client"
      ],
      "metadata": {
        "id": "s0WgMS2ozMar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, there's a lot of boilerplate in a standard HTTP request. OpenLLM includes both a CLI and Python client for making requests to a server."
      ],
      "metadata": {
        "id": "sNVlkpwGswGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openllm query --endpoint {SERVER_URL} --timeout 120 \"What is the weight of the earth?\"\n",
        "# os.environ[\"OPENLLM_ENDPOINT\"] = SERVER_URL\n",
        "# !openllm query --endpoint --timeout 120 \"What is the weight of the earth?\" --sample-params max-new-tokens=8192"
      ],
      "metadata": {
        "id": "lGu6rcjRtpC7",
        "outputId": "400ea26a-7805-49c6-c3f9-be1c5bb5efe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36mHello! \u001b[0m\u001b[36mI'm \u001b[0m\u001b[36mhappy \u001b[0m\u001b[36mto \u001b[0m\u001b[36mhelp \u001b[0m\u001b[36myou \u001b[0m\u001b[36mwith \u001b[0m\u001b[36myour \u001b[0m\u001b[36mquestion. \u001b[0m\u001b[36mHowever, \u001b[0m\u001b[36mI \u001b[0m\u001b[36mwant \u001b[0m\u001b[36mto \u001b[0m\u001b[36mpoint \u001b[0m\u001b[36mout \u001b[0m\u001b[36mthat \u001b[0m\u001b[36mthe \u001b[0m\u001b[36mquestion \u001b[0m\u001b[36m\"What \u001b[0m\u001b[36mis \u001b[0m\u001b[36mthe \u001b[0m\u001b[36mweight \u001b[0m\u001b[36mof \u001b[0m\u001b[36mthe \u001b[0m\u001b[36mearth?\" \u001b[0m\u001b[36mis \u001b[0m\u001b[36mnot \u001b[0m\u001b[36mfactually \u001b[0m\u001b[36mcoherent. \u001b[0m\u001b[36mThe \u001b[0m\u001b[36mEarth \u001b[0m\u001b[36mis \u001b[0m\u001b[36ma \u001b[0m\u001b[36mplanet, \u001b[0m\u001b[36mand \u001b[0m\u001b[36mit \u001b[0m\u001b[36mdoes \u001b[0m\u001b[36mnot \u001b[0m\u001b[36mhave \u001b[0m\u001b[36ma \u001b[0m\u001b[36mweight \u001b[0m\u001b[36mas \u001b[0m\u001b[36mit \u001b[0m\u001b[36mis \u001b[0m\u001b[36mnot \u001b[0m\u001b[36man \u001b[0m\u001b[36mobject \u001b[0m\u001b[36mthat \u001b[0m\u001b[36mcan \u001b[0m\u001b[36mbe \u001b[0m\u001b[36mweighed. \u001b[0m\u001b[36mThe \u001b[0m\u001b[36mEarth's \u001b[0m\u001b[36mmass \u001b[0m\u001b[36mis \u001b[0m\u001b[36mapproximately \u001b[0m\u001b[36m5.972 \u001b[0m\u001b[36mx \u001b[0m\u001b[36m10^24 \u001b[0m\u001b[36mkilograms, \u001b[0m\u001b[36mbut \u001b[0m\u001b[36mthis \u001b[0m\u001b[36mis \u001b[0m\u001b[36ma \u001b[0m\u001b[36mmeasure \u001b[0m\u001b[36mof \u001b[0m\u001b[36mits \u001b[0m\u001b[36mtotal \u001b[0m\u001b[36mmass, \u001b[0m\u001b[36mnot \u001b[0m\u001b[36mits \u001b[0m\u001b[36mweight.\n",
            "\n",
            "If \u001b[0m\u001b[36myou \u001b[0m\u001b[36mhave \u001b[0m\u001b[36many \u001b[0m\u001b[36mother \u001b[0m\u001b[36mquestions \u001b[0m\u001b[36mor \u001b[0m\u001b[36mif \u001b[0m\u001b[36mthere's \u001b[0m\u001b[36manything \u001b[0m\u001b[36melse \u001b[0m\u001b[36mI \u001b[0m\u001b[36mcan \u001b[0m\u001b[36mhelp \u001b[0m\u001b[36mwith, \u001b[0m\u001b[36mplease \u001b[0m\u001b[36mfeel \u001b[0m\u001b[36mfree \u001b[0m\u001b[36mto \u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openllm\n",
        "\n",
        "# sync API\n",
        "# client = openllm.client.HTTPClient(SERVER_URL)\n",
        "# res = client.query(\"what is the weight of the earth?\", max_new_tokens=8192)\n",
        "\n",
        "async_client = openllm.client.AsyncHTTPClient(SERVER_URL, timeout=120)\n",
        "res = await async_client.query(\"what is the weight of the earth?\", max_new_tokens=8192)\n",
        "print(res.responses[0])"
      ],
      "metadata": {
        "id": "dPR8VVZIs-M6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2a48526-9414-4c94-f7b6-d5880c3893c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The weight of the Earth is approximately 5.972 x 10^24 kilograms. This is based on the Earth's mass, which is estimated to be around 5.972 x 10^24 kilograms, and its density, which is approximately 5.514 g/cm^3. However, it's important to note that the Earth's weight is not a fixed value, as it can vary slightly due to the planet's slightly oblate shape and the effects of gravitational forces on its mass. Additionally, it's important to note that this value is an estimate based on scientific measurements and calculations, and it may not be exact or up-to-date.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an example of creating a back-and forth chat using LLaMA2's chat format:"
      ],
      "metadata": {
        "id": "01gXfUZCEJHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = []\n",
        "\n",
        "while True:\n",
        "  message = input(\"User: \")\n",
        "  if messages:\n",
        "    prompt = \"</s><s>[INST] \".join(messages) + \"</s><s>[INST] \" + message\n",
        "  else:\n",
        "    prompt = message\n",
        "  resp = await async_client.query(prompt, max_new_tokens=8192)\n",
        "  print(f\"Llama2: {resp.responses[0]}\")\n",
        "  messages.append(f\"{message} [/INST] {resp.responses[0]}\")"
      ],
      "metadata": {
        "id": "F1fptbjcEH2S",
        "outputId": "f4460754-1f0d-46e5-f559-55a8bac532d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI compatibility"
      ],
      "metadata": {
        "id": "rJV1BQ3azl-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we have just recently added OpenAI-compatible endpoints, so you can seamlessly port your OpenAI application to use OpenLLM by simply setting `openai.api_base`!"
      ],
      "metadata": {
        "id": "-3Y82FGGzn-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_base = SERVER_URL + \"/v1\"\n",
        "openai.api_key = \"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write me a haiku\\n\"}\n",
        "  ]\n",
        "chat = openai.ChatCompletion.create(model=\"llama2\", messages=messages)\n",
        "\n",
        "print(chat.choices[0].message.content)"
      ],
      "metadata": {
        "id": "0TnuwvC5gojG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BentoML"
      ],
      "metadata": {
        "id": "68SaNbJqz_QL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenLLM allows you to easily integrate it as part of a multi-modal application using BentoML. Here's a short example of what that might look like:"
      ],
      "metadata": {
        "id": "F4kOOSHg0HtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import asyncio\n",
        "\n",
        "import bentoml\n",
        "import openllm\n",
        "\n",
        "model = \"llama\"\n",
        "\n",
        "llm_config = openllm.AutoConfig.for_model(model)\n",
        "llm_runner = openllm.Runner(model, llm_config=llm_config)\n",
        "\n",
        "other_runner = bentoml.sklearn.get(\"my_cool_model:latest\").to_runner()\n",
        "\n",
        "svc = bentoml.Service(name=\"llm-service\", runners=[llm_runner, other_runner])\n",
        "\n",
        "@svc.api(input=bentoml.io.Text(), output=bentoml.io.Text())\n",
        "async def prompt(input_text: str) -> str:\n",
        "  llm_answer, other_answer = await asyncio.gather(\n",
        "      llm_runner.generate.async_run(input_text),\n",
        "      other_runner.classify(input_text),\n",
        "  )\n",
        "  return llm_answer[0][\"generated_text\"] + f\"\\nclassified as: {other_answer}\""
      ],
      "metadata": {
        "id": "VaCvP9ge0Q_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then take this Bento you have created and upload it to BentoCloud and deploy it at scale!\n",
        "\n",
        "```bash\n",
        "bentoml build\n",
        "bentoml push example_bento\n",
        "```"
      ],
      "metadata": {
        "id": "TI8F9IZz2UEu"
      }
    }
  ]
}