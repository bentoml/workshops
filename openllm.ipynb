{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "e-UHcNxx1Jyj",
        "HeN9bkZR2-Pj",
        "aAjLRuoGA2_y",
        "DwZCqUDQzE56",
        "o2J9PlcezIzG",
        "s0WgMS2ozMar",
        "kb5lX1y5zQ_K",
        "rJV1BQ3azl-t",
        "68SaNbJqz_QL"
      ],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bentoml/workshops/blob/main/openllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center>\n",
        "    <p style=\"text-align:center\">\n",
        "        <h1>OpenLLM</h1>\n",
        "        <img alt=\"BentoML logo\" src=\"https://raw.githubusercontent.com/bentoml/BentoML/main/docs/source/_static/img/bentoml-logo-black.png\" width=\"200\"/>\n",
        "        </br>\n",
        "        <a href=\"https://github.com/bentoml/OpenLLM\">GitHub</a>\n",
        "        |\n",
        "        <a href=\"https://l.bentoml.com/join-openllm-discord\">Community</a>\n",
        "    </p>\n",
        "</center>\n",
        "<h1 align=\"center\">Serving an open-source LLM with OpenLLM</h1>\n",
        "\n",
        "Open-source LLMs can be very powerful and flexible, but setting them up for serving can be quite difficult. Indeed, the vast landscape of LLMs means that you may have to do the same process for several models in order to evaluate their performance. With OpenLLM, you can adapt and serve many models (including Llama 2 and Falcon) with ease.\n",
        "\n",
        "In this tutorial, you will learn the following:\n",
        "\n",
        "- Set up your environment to work with OpenLLM.\n",
        "- Serve LLMs like Llama 2 with just a single command.\n",
        "- Explore different ways to interact with the OpenLLM server.\n",
        "- Advanced features of OpenLLM like quantization.\n",
        "- Integrate OpenLLM with LangChain.\n",
        "- Port existing OpenAI applications to use OpenLLM with minimal code changes.\n",
        "- Create a multi-modal application with OpenLLM and BentoML."
      ],
      "metadata": {
        "id": "Ae3XDcmOCraD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "LVSRXlTJteuK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into OpenLLM, let's ensure our environment has everything in place."
      ],
      "metadata": {
        "id": "AKL2j904kqGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RUN_IN_COLAB = False #@param {type: 'boolean'}\n",
        "SERVER_URL = \"http://llama-13b-org-ss-org-1--aws-us-east-1.mt2.bentoml.ai\"\n",
        "\n",
        "print(\"Installing OpenLLM...\")\n",
        "!pip install --upgrade -q --progress-bar off openllm bentoml langchain openai\n",
        "print(\"Done!\")\n",
        "\n",
        "if RUN_IN_COLAB:\n",
        "  SERVER_URL = \"http://localhost:8001\"\n",
        "  print(\"Installing serving dependencies...\")\n",
        "  !pip install --upgrade -q --progress-bar off openllm[llama,vllm,gptq] tensorrt accelerate bitsandbytes\n",
        "  !apt install tensorrt\n",
        "  print(\"Done!\")\n",
        "\n",
        "  print(\"Downloading Llama and starting OpenLLM server...\")\n",
        "  !nohup openllm start llama --model-id \"NousResearch/llama-2-7b-chat-hf\" --backend vllm --port 8001 > openllm.log 2> openllm.err &\n",
        "\n",
        "import os\n",
        "os.environ[\"SERVER_URL\"] = SERVER_URL\n"
      ],
      "metadata": {
        "id": "vfbUDJzFtke-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab8d2e3a-01af-4956-ca68-41c586184108"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing OpenLLM...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for optimum (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [Optional] Check GPU and memory resources"
      ],
      "metadata": {
        "id": "e-UHcNxx1Jyj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Use the following script to check the GPU and memory resources in your environment."
      ],
      "metadata": {
        "id": "06gxHpky-SGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "import torch\n",
        "\n",
        "ram = psutil.virtual_memory()\n",
        "ram_total = ram.total / (1024 ** 3)\n",
        "print(\"MemTotal: %.2f GB\" % ram_total)\n",
        "\n",
        "print(\"=============GPU INFO=============\")\n",
        "if torch.cuda.is_available():\n",
        "    !/opt/bin/nvidia-smi || true\n",
        "else:\n",
        "    print(\"GPU NOT available\")\n",
        "    print(\"Run `openllm models` to find models which are executable on CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qESpen0eufgx",
        "outputId": "44adf929-ac41-46c6-967e-2a2c612051bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal: 12.68 GB\n",
            "=============GPU INFO=============\n",
            "Thu Oct 12 10:04:18 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8    10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serve Llama 2!\n",
        "\n",
        "While we won't be serving Llama 2 from inside the Colab environment, serving it is straightforward with OpenLLM. With just a single command, you're good to go:\n",
        "\n",
        "```bash\n",
        "openllm start llama --model-id \"NousResearch/llama-2-7b-chat-hf\" --backend vllm --port 8001\n",
        "```\n",
        "\n",
        "If you have access to a GPU environment, try it out!"
      ],
      "metadata": {
        "id": "UOrB4TU20Zl0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we're serving the smallest Llama 2 model, the 7 billion parameter version. You can simply change `7b` in the string with `13b` or `70b` for the larger parameter sizes."
      ],
      "metadata": {
        "id": "bimBS4uv2UBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View OpenLLM model options"
      ],
      "metadata": {
        "id": "HeN9bkZR2-Pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenLLM offers a wide range of models and backends. To see the available options, simply run `openllm models`:"
      ],
      "metadata": {
        "id": "HlsRN4TWmUpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openllm models"
      ],
      "metadata": {
        "id": "kvIBUYE-2YoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bbbab46-f7b2-4b76-d0eb-ee2bdaebbebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[37m╒═══════════╤══════════════════════╤════════════════════════════════╤══════════════════════╤══════════════════════════════╕\n",
            "│ LLM       │ Architecture         │ Models Id                      │ Installation         │ Runtime                      │\n",
            "╞═══════════╪══════════════════════╪════════════════════════════════╪══════════════════════╪══════════════════════════════╡\n",
            "│ chatglm   │ ChatGLMForConditiona │ ['thudm/chatglm-6b',           │ \"openllm[chatglm]\"   │ ('pt',)                      │\n",
            "│           │ lGeneration          │ 'thudm/chatglm-6b-int8',       │                      │                              │\n",
            "│           │                      │ 'thudm/chatglm-6b-int4',       │                      │                              │\n",
            "│           │                      │ 'thudm/chatglm2-6b',           │                      │                              │\n",
            "│           │                      │ 'thudm/chatglm2-6b-int4']      │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ dolly-v2  │ GPTNeoXForCausalLM   │ ['databricks/dolly-v2-3b',     │ openllm              │ ('pt', 'vllm')               │\n",
            "│           │                      │ 'databricks/dolly-v2-7b',      │                      │                              │\n",
            "│           │                      │ 'databricks/dolly-v2-12b']     │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ falcon    │ FalconForCausalLM    │ ['tiiuae/falcon-7b',           │ \"openllm[falcon]\"    │ ('pt', 'vllm')               │\n",
            "│           │                      │ 'tiiuae/falcon-40b',           │                      │                              │\n",
            "│           │                      │ 'tiiuae/falcon-7b-instruct',   │                      │                              │\n",
            "│           │                      │ 'tiiuae/falcon-40b-instruct']  │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ flan-t5   │ T5ForConditionalGene │ ['google/flan-t5-small',       │ \"openllm[flan-t5]\"   │ ('pt', 'flax', 'tf')         │\n",
            "│           │ ration               │ 'google/flan-t5-base',         │                      │                              │\n",
            "│           │                      │ 'google/flan-t5-large',        │                      │                              │\n",
            "│           │                      │ 'google/flan-t5-xl',           │                      │                              │\n",
            "│           │                      │ 'google/flan-t5-xxl']          │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ gpt-neox  │ GPTNeoXForCausalLM   │ ['eleutherai/gpt-neox-20b']    │ openllm              │ ('pt', 'vllm')               │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ llama     │ LlamaForCausalLM     │ ['meta-llama/Llama-2-70b-chat- │ \"openllm[llama]\"     │ ('pt', 'vllm')               │\n",
            "│           │                      │ hf', 'meta-                    │                      │                              │\n",
            "│           │                      │ llama/Llama-2-13b-chat-hf',    │                      │                              │\n",
            "│           │                      │ 'meta-llama/Llama-2-7b-chat-   │                      │                              │\n",
            "│           │                      │ hf', 'meta-                    │                      │                              │\n",
            "│           │                      │ llama/Llama-2-70b-hf', 'meta-  │                      │                              │\n",
            "│           │                      │ llama/Llama-2-13b-hf', 'meta-  │                      │                              │\n",
            "│           │                      │ llama/Llama-2-7b-hf', 'NousRes │                      │                              │\n",
            "│           │                      │ earch/llama-2-70b-chat-hf', 'N │                      │                              │\n",
            "│           │                      │ ousResearch/llama-2-13b-chat-  │                      │                              │\n",
            "│           │                      │ hf',                           │                      │                              │\n",
            "│           │                      │ 'NousResearch/llama-2-7b-chat- │                      │                              │\n",
            "│           │                      │ hf',                           │                      │                              │\n",
            "│           │                      │ 'NousResearch/llama-2-70b-hf', │                      │                              │\n",
            "│           │                      │ 'NousResearch/llama-2-13b-hf', │                      │                              │\n",
            "│           │                      │ 'NousResearch/llama-2-7b-hf']  │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ mpt       │ MPTForCausalLM       │ ['mosaicml/mpt-7b',            │ \"openllm[mpt]\"       │ ('pt', 'vllm')               │\n",
            "│           │                      │ 'mosaicml/mpt-7b-instruct',    │                      │                              │\n",
            "│           │                      │ 'mosaicml/mpt-7b-chat',        │                      │                              │\n",
            "│           │                      │ 'mosaicml/mpt-7b-storywriter', │                      │                              │\n",
            "│           │                      │ 'mosaicml/mpt-30b',            │                      │                              │\n",
            "│           │                      │ 'mosaicml/mpt-30b-instruct',   │                      │                              │\n",
            "│           │                      │ 'mosaicml/mpt-30b-chat']       │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ opt       │ OPTForCausalLM       │ ['facebook/opt-125m',          │ \"openllm[opt]\"       │ ('pt', 'flax', 'tf', 'vllm') │\n",
            "│           │                      │ 'facebook/opt-350m',           │                      │                              │\n",
            "│           │                      │ 'facebook/opt-1.3b',           │                      │                              │\n",
            "│           │                      │ 'facebook/opt-2.7b',           │                      │                              │\n",
            "│           │                      │ 'facebook/opt-6.7b',           │                      │                              │\n",
            "│           │                      │ 'facebook/opt-66b']            │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ stablelm  │ GPTNeoXForCausalLM   │ ['stabilityai/stablelm-tuned-  │ openllm              │ ('pt', 'vllm')               │\n",
            "│           │                      │ alpha-3b',                     │                      │                              │\n",
            "│           │                      │ 'stabilityai/stablelm-tuned-   │                      │                              │\n",
            "│           │                      │ alpha-7b',                     │                      │                              │\n",
            "│           │                      │ 'stabilityai/stablelm-base-    │                      │                              │\n",
            "│           │                      │ alpha-3b',                     │                      │                              │\n",
            "│           │                      │ 'stabilityai/stablelm-base-    │                      │                              │\n",
            "│           │                      │ alpha-7b']                     │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ starcoder │ GPTBigCodeForCausalL │ ['bigcode/starcoder',          │ \"openllm[starcoder]\" │ ('pt', 'vllm')               │\n",
            "│           │ M                    │ 'bigcode/starcoderbase']       │                      │                              │\n",
            "├───────────┼──────────────────────┼────────────────────────────────┼──────────────────────┼──────────────────────────────┤\n",
            "│ baichuan  │ BaiChuanForCausalLM  │ ['baichuan-inc/baichuan-7b',   │ \"openllm[baichuan]\"  │ ('pt', 'vllm')               │\n",
            "│           │                      │ 'baichuan-                     │                      │                              │\n",
            "│           │                      │ inc/baichuan-13b-base',        │                      │                              │\n",
            "│           │                      │ 'baichuan-                     │                      │                              │\n",
            "│           │                      │ inc/baichuan-13b-chat',        │                      │                              │\n",
            "│           │                      │ 'fireballoon/baichuan-vicuna-  │                      │                              │\n",
            "│           │                      │ chinese-7b',                   │                      │                              │\n",
            "│           │                      │ 'fireballoon/baichuan-         │                      │                              │\n",
            "│           │                      │ vicuna-7b',                    │                      │                              │\n",
            "│           │                      │ 'hiyouga/baichuan-7b-sft']     │                      │                              │\n",
            "╘═══════════╧══════════════════════╧════════════════════════════════╧══════════════════════╧══════════════════════════════╛\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Serving with quantization\n",
        "\n",
        "While you can serve smaller models, like the 7 billion parameter Llama model in the example, you can't serve larger models, as the VRAM requirement is very large."
      ],
      "metadata": {
        "id": "aAjLRuoGA2_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title [Optional] Try to serve Llama 13b\n",
        "!openllm start llama --model-id \"NousResearch/llama-2-13b-chat-hf\" --port 8001"
      ],
      "metadata": {
        "id": "28Ej1fiID-_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to serve 13b on a GPU with limited VRAM, we can take advantage of [quantization](https://towardsdatascience.com/introduction-to-weight-quantization-2494701b9c0c). In essence, quantization rounds weight values, significantly lowering the VRAM required to hold the model in memory. You can start using quantization super easily with OpenLLM, by simply passing the `--quantize` argument. There are several options, including `int4` and `int8`, but for this workshop, we will be using the state-of-the-art [`gptq` quantization](https://arxiv.org/abs/2210.17323)."
      ],
      "metadata": {
        "id": "t8SuuAhTExkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!openllm start llama --model-id \"TheBloke/Llama-2-13b-Chat-GPTQ\" --quantize gptq"
      ],
      "metadata": {
        "id": "ZUGjrebmFUjm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a65ea1de-f8f3-45c0-e785-1f981b46b05c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m'--quantize=gptq' might not work with 'safetensors' serialisation format. To silence this warning, set \"OPENLLM_SERIALIZATION_WARNING=False\"\n",
            "Note: You can always fallback to '--serialisation legacy' when running quantisation.\u001b[0m\n",
            "\u001b[32mMake sure to check out 'TheBloke/Llama-2-13b-Chat-GPTQ' repository to see if the weights is in 'safetensors' format if unsure.\u001b[0m\n",
            "\u001b[33mMake sure to have the following dependencies available: ['fairscale']\u001b[0m\n",
            "'quantize=\"gptq\"' requires 'auto-gptq' and 'optimum>=0.12' to be installed (not available with local environment). Make sure to have 'auto-gptq' available locally: 'pip install \"openllm[gptq]\"'. OpenLLM will fallback to int8 with bitsandbytes.\n",
            "Downloading (…)lve/main/config.json: 100% 837/837 [00:00<00:00, 5.01MB/s]\n",
            "'quantize=\"gptq\"' requires 'auto-gptq' and 'optimum>=0.12' to be installed (not available with local environment). Make sure to have 'auto-gptq' available locally: 'pip install \"openllm[gptq]\"'. OpenLLM will fallback to int8 with bitsandbytes.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/serialisation/transformers/__init__.py\", line 147, in get\n",
            "    model = bentoml.models.get(llm.tag)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simple_di/__init__.py\", line 139, in _\n",
            "    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bentoml/models.py\", line 45, in get\n",
            "    return _model_store.get(tag)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bentoml/_internal/store.py\", line 146, in get\n",
            "    raise NotFound(\n",
            "bentoml.exceptions.NotFound: Model 'pt-thebloke--llama-2-13b-chat-gptq:ea078917a7e91c896787c73dba935f032ae658e9' is not found in BentoML store <osfs '/root/bentoml/models'>, you may need to run `bentoml models pull` first\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/cli/entrypoint.py\", line 407, in import_command\n",
            "    _ref = openllm.serialisation.get(llm)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/serialisation/__init__.py\", line 75, in caller\n",
            "    return getattr(importlib.import_module(f'.{serde}', __name__), fn)(llm, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/serialisation/transformers/__init__.py\", line 155, in get\n",
            "    raise openllm.exceptions.OpenLLMException(f'Failed while getting stored artefact (lookup for traceback):\\n{err}') from err\n",
            "openllm_core.exceptions.OpenLLMException: Failed while getting stored artefact (lookup for traceback):\n",
            "Model 'pt-thebloke--llama-2-13b-chat-gptq:ea078917a7e91c896787c73dba935f032ae658e9' is not found in BentoML store <osfs '/root/bentoml/models'>, you may need to run `bentoml models pull` first\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/serialisation/transformers/__init__.py\", line 147, in get\n",
            "    model = bentoml.models.get(llm.tag)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simple_di/__init__.py\", line 139, in _\n",
            "    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bentoml/models.py\", line 45, in get\n",
            "    return _model_store.get(tag)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bentoml/_internal/store.py\", line 146, in get\n",
            "    raise NotFound(\n",
            "bentoml.exceptions.NotFound: Model 'pt-thebloke--llama-2-13b-chat-gptq:ea078917a7e91c896787c73dba935f032ae658e9' is not found in BentoML store <osfs '/root/bentoml/models'>, you may need to run `bentoml models pull` first\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/openllm\", line 8, in <module>\n",
            "    sys.exit(cli())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1688, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/cli/entrypoint.py\", line 191, in wrapper\n",
            "    return_value = func(*args, **attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/decorators.py\", line 33, in new_func\n",
            "    return f(get_current_context(), *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/cli/entrypoint.py\", line 173, in wrapper\n",
            "    return f(*args, **attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/decorators.py\", line 33, in new_func\n",
            "    return f(get_current_context(), *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/cli/_factory.py\", line 189, in start_cmd\n",
            "    llm = openllm.utils.infer_auto_class(env['backend_value']).for_model(model,\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/models/auto/factory.py\", line 52, in for_model\n",
            "    if ensure_available: llm.save_pretrained()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/_llm.py\", line 711, in save_pretrained\n",
            "    return openllm.import_model(self.config['start_name'],\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/cli/_sdk.py\", line 250, in _import_model\n",
            "    return import_command.main(args=args, standalone_mode=False)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\n",
            "    rv = self.invoke(ctx)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\n",
            "    return __callback(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/cli/entrypoint.py\", line 191, in wrapper\n",
            "    return_value = func(*args, **attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/click/decorators.py\", line 33, in new_func\n",
            "    return f(get_current_context(), *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/cli/entrypoint.py\", line 173, in wrapper\n",
            "    return f(*args, **attrs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/cli/entrypoint.py\", line 413, in import_command\n",
            "    _ref = openllm.serialisation.get(llm, auto_import=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/serialisation/__init__.py\", line 75, in caller\n",
            "    return getattr(importlib.import_module(f'.{serde}', __name__), fn)(llm, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/serialisation/transformers/__init__.py\", line 154, in get\n",
            "    if auto_import: return import_model(llm, trust_remote_code=llm.trust_remote_code)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/simple_di/__init__.py\", line 139, in _\n",
            "    return func(*_inject_args(bind.args), **_inject_kwargs(bind.kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/openllm/serialisation/transformers/__init__.py\", line 79, in import_model\n",
            "    raise openllm.exceptions.OpenLLMException(\n",
            "openllm_core.exceptions.OpenLLMException: GPTQ quantisation requires 'auto-gptq' and 'optimum' (Not found in local environment). Install it with 'pip install \"openllm[gptq]\" --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the OpenLLM server"
      ],
      "metadata": {
        "id": "tvWmNvppjU6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check server status"
      ],
      "metadata": {
        "id": "DwZCqUDQzE56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before you interact with the OpenLLM server, it's crucial to ensure that it is up and running. The output of the `curl` command should start with `HTTP/1.1 200 OK`, meaning everything is in order.\n",
        "\n",
        "If it says `curl: (6) Could not resolve host: SERVER_URL`, ensure you have run the setup step.\n",
        "\n",
        "If it says `curl: (7) Failed to connect to localhost...`, then check `./openllm.log` and `./openllm.err`; likely the server has failed to start or is still in the process of starting.\n",
        "\n",
        "If it says `HTTP/1.1 503 Service Unavailable`, the server is still starting and you should wait a bit and retry."
      ],
      "metadata": {
        "id": "YmAl7HYgnDBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -i {SERVER_URL}/readyz"
      ],
      "metadata": {
        "id": "SPVPsGp2nFS5",
        "outputId": "f3c318bb-5e91-47c9-c7c9-fb234e477535",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HTTP/1.1 200 OK\r\n",
            "\u001b[1mDate\u001b[0m: Thu, 12 Oct 2023 09:55:29 GMT\r\n",
            "\u001b[1mContent-Type\u001b[0m: text/plain; charset=utf-8\r\n",
            "\u001b[1mContent-Length\u001b[0m: 1\r\n",
            "\u001b[1mConnection\u001b[0m: keep-alive\r\n",
            "\u001b[1mX-Powered-By\u001b[0m: Yatai\r\n",
            "\u001b[1mX-Yatai-Org-Name\u001b[0m: unknown\r\n",
            "\u001b[1mX-Yatai-Bento\u001b[0m: meta-llama--llama-2-13b-chat-hf-service:0ba94ac9b9e1d5a0037780667e8b219adde1908c\r\n",
            "\r\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Raw HTTP"
      ],
      "metadata": {
        "id": "o2J9PlcezIzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several ways you can interact with an OpenLLM server. Since it is a standard HTTP server that accepts JSON as input, you can simply use `cURL` (or any HTTP client of your choice):"
      ],
      "metadata": {
        "id": "Azr44vD_jc48"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -k -X 'POST' \\\n",
        "  \"$SERVER_URL/v1/generate_stream\" \\\n",
        "  -H 'accept: text/event-stream' \\\n",
        "  -H 'Content-Type: application/json' \\\n",
        "  -d '{\"prompt\":\"write a tagline for an ice cream shop\\n\", \"llm_config\": {\"max_new_tokens\": 8192}}'"
      ],
      "metadata": {
        "id": "3rwArmgCm_j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenLLM client"
      ],
      "metadata": {
        "id": "s0WgMS2ozMar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, there's a lot of boilerplate in a standard HTTP request. OpenLLM includes both a CLI and Python client for making requests to a server."
      ],
      "metadata": {
        "id": "sNVlkpwGswGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openllm\n",
        "client = openllm.client.HTTPClient(SERVER_URL)\n",
        "\n",
        "res = await client.query(\"what is the weight of the earth?\", return_response='raw')\n",
        "print(res)"
      ],
      "metadata": {
        "id": "dPR8VVZIs-M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!openllm query --endpoint {SERVER_URL} --timeout 120 \"What is the weight of the earth?\""
      ],
      "metadata": {
        "id": "lGu6rcjRtpC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LangChain integration"
      ],
      "metadata": {
        "id": "kb5lX1y5zQ_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenLLM also integrates with LangChain, allowing for easy use if you're familiar with LangChain APIs. It also allows you to integrate with any other LangChain-supported LLMs and APIs."
      ],
      "metadata": {
        "id": "7a10Z5W-vWDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenLLM\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"country\"],\n",
        "    template=\"What are some phrases that would be useful to know when visiting {country}?\",\n",
        ")\n",
        "\n",
        "llm = OpenLLM(server_url=SERVER_URL)\n",
        "\n",
        "llmchain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "country = input(\"Country: \")\n",
        "llmchain.run(country)"
      ],
      "metadata": {
        "id": "UM-Ge6oEsz24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI compatibility"
      ],
      "metadata": {
        "id": "rJV1BQ3azl-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we have just recently added OpenAI-compatible endpoints, so you can seamlessly port your OpenAI application to use OpenLLM by simply setting `openai.api_base`!"
      ],
      "metadata": {
        "id": "-3Y82FGGzn-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_base = SERVER_URL + \"/v1\"\n",
        "openai.api_key = \"\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write me a haiku\\n\"}\n",
        "  ]\n",
        "chat = openai.ChatCompletion.create(model=\"llama2\", messages=messages)\n",
        "\n",
        "print(f\"{chat.choices[0].message.content}\")"
      ],
      "metadata": {
        "id": "0TnuwvC5gojG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "af44bc60-6820-451b-dff7-0cfdcbc5db73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3235ef34931d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSERVER_URL\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/v1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m messages = [\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BentoML"
      ],
      "metadata": {
        "id": "68SaNbJqz_QL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OpenLLM allows you to easily integrate it as part of a multi-modal application using BentoML. Here's a short example of what that might look like:"
      ],
      "metadata": {
        "id": "F4kOOSHg0HtD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import bentoml\n",
        "import openllm\n",
        "\n",
        "model = \"llama\"\n",
        "\n",
        "llm_config = openllm.AutoConfig.for_model(model)\n",
        "llm_runner = openllm.Runner(model, llm_config=llm_config)\n",
        "\n",
        "other_runner = bentoml.sklearn.get(\"my_cool_model:latest\").to_runner()\n",
        "\n",
        "svc = bentoml.Service(name=\"llm-service\", runners=[llm_runner, other_runner])\n",
        "\n",
        "@svc.api(input=bentoml.io.Text(), output=bentoml.io.Text())\n",
        "async def prompt(input_text: str) -> str:\n",
        "  llm_answer, other_answer = await asyncio.gather(\n",
        "      llm_runner.generate.async_run(input_text),\n",
        "      other_runner.classify(tokenize(other_answer))\n",
        "  )\n",
        "  return answer[0][\"generated_text\"] + f\"\\nclassified as: {other_answer}\""
      ],
      "metadata": {
        "id": "VaCvP9ge0Q_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can then take this Bento you have created and upload it to BentoCloud and deploy it at scale!\n",
        "\n",
        "```bash\n",
        "bentoml build\n",
        "bentoml push example_bento\n",
        "```"
      ],
      "metadata": {
        "id": "TI8F9IZz2UEu"
      }
    }
  ]
}